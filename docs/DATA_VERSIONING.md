# Data Versioning and Reproducibility

## Versioning approach
- **Method:** The dataset in `dailydrip_rag/data` is static sample data captured from real pour-over brew sessions, so we rely on Git for version control. Every change to the raw logs or processed outputs is committed alongside code changes, and we establish a semantic dataset version by publishing Git tags (e.g., `data-v1.0`) when we ship a curated snapshot.
- **Justification:** Static data, like our curated brew logs, does not require heavy tooling such as DVC; Git already captures the diff, provenance, and history. This keeps the workflow lightweight and accessible while providing the exact file contents for any commit.

## Dataset inventory & history
> The data currently stored in `dailydrip_rag/data` is a proof-of-concept snapshot of the pour-over sessions our team used to shape the agent. As the dataset matures, future versions will capture the benefits of our finalized brew-planning guidance.
| Dataset | Location | Current version | Notes |
| --- | --- | --- | --- |
| Brew log samples | `dailydrip_rag/data/raw/coffee_brew_logs_sample.jsonl` | `1.0` (Git commit `1e26c4c`) | Original pour-over & drip session notes that seeded the agent’s knowledge base. |
| Processed records | `dailydrip_rag/data/processed/records.jsonl` | `1.0` (Git commit `1e26c4c`) | Structured records generated by the ingestion pipeline from the raw log. |
| Embedding chunks | `dailydrip_rag/data/processed/chunks.jsonl` | `1.0` (Git commit `1e26c4c`) | Chunked passages derived from `records.jsonl` for later indexing in a vector database. |

Future dataset updates should follow the same pattern: edit or add files under `dailydrip_rag/data`, run any preprocessing script (e.g., `scripts/chunk.py`), confirm the outputs, commit the changes, and tag the commit (e.g., `git tag data-v1.1`) to mark a new snapshot.

## Retrieval instructions
1. Clone or pull the repository to get the latest data: `git pull` at the repository root.
2. Checkout a specific dataset version if needed: `git checkout <commit-or-tag> -- dailydrip_rag/data`.
3. After checking out, run the ingestion pipeline (if you need to re-generate processed files) so the artifacts match the commit—this ensures reproducibility alongside the tracked data.

## Reproducibility notes
- The `records.jsonl` and `chunks.jsonl` files are derived from `coffee_brew_logs_sample.jsonl`. Re-run the ingestion scripts listed in `README.md` or `dailydrip_rag/README.md` (if any exist) to re-create these artifacts from scratch.
- Keep a short changelog entry when altering the raw logs to remind future maintainers which pour-over experiments led to the new dataset version, and note that the current repository contents are a proof-of-concept; the team is actively refining these samples to demonstrate the final pour-over recipe planning benefits.

## LLM provenance
- No LLM-generated data is shipped in `dailydrip_rag/data`. The samples are human-written, so there are no prompts or outputs to document at this time.
